{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bill080804/vision-transformer-form-scratch?scriptVersionId=280203102\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"'''basic imports   '''\n'''https://docs.pytorch.org/docs/stable/index.html'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:27:00.11903Z","iopub.execute_input":"2025-11-18T08:27:00.119282Z","iopub.status.idle":"2025-11-18T08:27:12.447204Z","shell.execute_reply.started":"2025-11-18T08:27:00.119261Z","shell.execute_reply":"2025-11-18T08:27:12.445714Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\n'''for testing gpu l4 wala'''\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:27:31.456802Z","iopub.execute_input":"2025-11-18T08:27:31.457131Z","iopub.status.idle":"2025-11-18T08:27:31.466715Z","shell.execute_reply.started":"2025-11-18T08:27:31.457106Z","shell.execute_reply":"2025-11-18T08:27:31.465582Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nrandom.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:27:12.458755Z","iopub.status.idle":"2025-11-18T08:27:12.459116Z","shell.execute_reply.started":"2025-11-18T08:27:12.458937Z","shell.execute_reply":"2025-11-18T08:27:12.458951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 128 '''will tune hyperpyrameter later for now will use this openly avilable paramaters'''\nEPOCHS = 10'''ya chage mat karna for now'''\nLEARNING_RATE = 3e-4\nPATCH_SIZE = 4 '''in a vision transformer we don't make a vector cuz it is too computationally expensive to perofrm that task so we make patches small sub structures of those images.'''\nNUM_CLASSES = 10\nIMAGE_SIZE = 32\nCHANNELS = 3\nEMBED_DIM = 256\nNUM_HEADS = 8\nDEPTH = 6\nMLP_DIM = 512\nDROP_RATE = 0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T08:27:12.460486Z","iopub.status.idle":"2025-11-18T08:27:12.461069Z","shell.execute_reply.started":"2025-11-18T08:27:12.460896Z","shell.execute_reply":"2025-11-18T08:27:12.460917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data preprocessing\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_dataset = datasets.CIFAR10(root='path', train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10(root='path', train=False, download=True, transform=transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        num_patches = (img_size // patch_size) ** 2\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n\n    def forward(self, x):\n        B = x.size(0)\n        x = self.proj(x)  # (B, E, H/P, W/P)\n        x = x.flatten(2).transpose(1, 2)  # (B, N, E)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, in_features, hidden_features, drop_rate):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, in_features)\n        self.dropout = nn.Dropout(drop_rate)\n\n    def forward(self, x):\n        x = self.dropout(F.gelu(self.fc1(x)))\n        x = self.dropout(self.fc2(x))\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        self.encoder = nn.Sequential(*[\n            TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n            for _ in range(depth)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = self.encoder(x)\n        x = self.norm(x)\n        cls_token = x[:, 0]\n        return self.head(cls_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate model\nmodel = VisionTransformer(\n    IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES,\n    EMBED_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROP_RATE\n).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\ndef train(model, loader, optimizer, criterion):\n    model.train()\n    total_loss, correct = 0, 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * x.size(0)\n        correct += (out.argmax(1) == y).sum().item()\n    return total_loss / len(loader.dataset), correct / len(loader.dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation loop\ndef evaluate(model, loader):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            correct += (out.argmax(1) == y).sum().item()\n    return correct / len(loader.dataset)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\ntrain_accuracies, test_accuracies = [], []\nfor epoch in range(EPOCHS):\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    test_acc = evaluate(model, test_loader)\n    train_accuracies.append(train_acc)\n    test_accuracies.append(test_acc)\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Plot accuracy\nplt.plot(train_accuracies, label='Train Accuracy')\nplt.plot(test_accuracies, label='Test Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training and Test Accuracy')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}